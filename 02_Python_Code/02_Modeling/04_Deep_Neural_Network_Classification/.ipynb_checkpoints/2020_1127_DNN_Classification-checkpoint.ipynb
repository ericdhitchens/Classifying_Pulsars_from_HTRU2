{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Pulsars from the High Time Resolution Universe Survey (HTRU2) - Deep Neural Network (DNN) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview & Citation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code notebook, we attempt to classify pulsars from the High Time Resolution Universe Survey, South (HTRU2) dataset using deep neural network (DNN) classification. The dataset was retrieved from the UC Irvine Machine Learning Repository at the following link: https://archive.ics.uci.edu/ml/datasets/HTRU2#.\n",
    "\n",
    "The dataset was donated to the UCI Repository by Dr. Robert Lyon of The University of Manchester, United Kingdom. The two papers requested for citation in the description are listed below:\n",
    "\n",
    "* R. J. Lyon, B. W. Stappers, S. Cooper, J. M. Brooke, J. D. Knowles, Fifty Years of Pulsar Candidate Selection: From simple filters to a new principled real-time classification approach, Monthly Notices of the Royal Astronomical Society 459 (1), 1104-1123, DOI: 10.1093/mnras/stw656\n",
    "* R. J. Lyon, HTRU2, DOI: 10.6084/m9.figshare.3080389.v1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ANN Modeling in TensorFlow & Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import & Check the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2020_1125_Pulsar_Data.csv')\n",
    "pulsar_data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IP_Mean</th>\n",
       "      <th>IP_StdDev</th>\n",
       "      <th>IP_Kurtosis</th>\n",
       "      <th>IP_Skewness</th>\n",
       "      <th>DM_Mean</th>\n",
       "      <th>DM_StdDev</th>\n",
       "      <th>DM_Kurtosis</th>\n",
       "      <th>DM_Skewness</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140.562500</td>\n",
       "      <td>55.683782</td>\n",
       "      <td>-0.234571</td>\n",
       "      <td>-0.699648</td>\n",
       "      <td>3.199833</td>\n",
       "      <td>19.110426</td>\n",
       "      <td>7.975532</td>\n",
       "      <td>74.242225</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102.507812</td>\n",
       "      <td>58.882430</td>\n",
       "      <td>0.465318</td>\n",
       "      <td>-0.515088</td>\n",
       "      <td>1.677258</td>\n",
       "      <td>14.860146</td>\n",
       "      <td>10.576487</td>\n",
       "      <td>127.393580</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.015625</td>\n",
       "      <td>39.341649</td>\n",
       "      <td>0.323328</td>\n",
       "      <td>1.051164</td>\n",
       "      <td>3.121237</td>\n",
       "      <td>21.744669</td>\n",
       "      <td>7.735822</td>\n",
       "      <td>63.171909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>136.750000</td>\n",
       "      <td>57.178449</td>\n",
       "      <td>-0.068415</td>\n",
       "      <td>-0.636238</td>\n",
       "      <td>3.642977</td>\n",
       "      <td>20.959280</td>\n",
       "      <td>6.896499</td>\n",
       "      <td>53.593661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.726562</td>\n",
       "      <td>40.672225</td>\n",
       "      <td>0.600866</td>\n",
       "      <td>1.123492</td>\n",
       "      <td>1.178930</td>\n",
       "      <td>11.468720</td>\n",
       "      <td>14.269573</td>\n",
       "      <td>252.567306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      IP_Mean  IP_StdDev  IP_Kurtosis  IP_Skewness   DM_Mean  DM_StdDev  \\\n",
       "0  140.562500  55.683782    -0.234571    -0.699648  3.199833  19.110426   \n",
       "1  102.507812  58.882430     0.465318    -0.515088  1.677258  14.860146   \n",
       "2  103.015625  39.341649     0.323328     1.051164  3.121237  21.744669   \n",
       "3  136.750000  57.178449    -0.068415    -0.636238  3.642977  20.959280   \n",
       "4   88.726562  40.672225     0.600866     1.123492  1.178930  11.468720   \n",
       "\n",
       "   DM_Kurtosis  DM_Skewness  Class  \n",
       "0     7.975532    74.242225      0  \n",
       "1    10.576487   127.393580      0  \n",
       "2     7.735822    63.171909      0  \n",
       "3     6.896499    53.593661      0  \n",
       "4    14.269573   252.567306      0  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pulsar_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pulsar_data.drop('Class',axis=1)\n",
    "y = pulsar_data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train= scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - DNN with 2 Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13423, 8)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine number of starting nodes by finding the shape of X_train\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model.add(Dense(8,activation='relu')) # All layers utilize rectified linear units (relu)\n",
    "\n",
    "# Hidden Layers\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "\n",
    "# Output Layer (Sigmoid for Binary Classification)\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "420/420 [==============================] - 0s 737us/step - loss: 0.3329 - val_loss: 0.1499\n",
      "Epoch 2/200\n",
      "420/420 [==============================] - 0s 588us/step - loss: 0.1032 - val_loss: 0.0866\n",
      "Epoch 3/200\n",
      "420/420 [==============================] - 0s 600us/step - loss: 0.0874 - val_loss: 0.0840\n",
      "Epoch 4/200\n",
      "420/420 [==============================] - 0s 616us/step - loss: 0.0850 - val_loss: 0.0838\n",
      "Epoch 5/200\n",
      "420/420 [==============================] - 0s 589us/step - loss: 0.0842 - val_loss: 0.0831\n",
      "Epoch 6/200\n",
      "420/420 [==============================] - 0s 586us/step - loss: 0.0836 - val_loss: 0.0833\n",
      "Epoch 7/200\n",
      "420/420 [==============================] - 0s 589us/step - loss: 0.0830 - val_loss: 0.0814\n",
      "Epoch 8/200\n",
      "420/420 [==============================] - 0s 593us/step - loss: 0.0823 - val_loss: 0.0814\n",
      "Epoch 9/200\n",
      "420/420 [==============================] - 0s 589us/step - loss: 0.0817 - val_loss: 0.0806\n",
      "Epoch 10/200\n",
      "420/420 [==============================] - 0s 590us/step - loss: 0.0811 - val_loss: 0.0803\n",
      "Epoch 11/200\n",
      "420/420 [==============================] - 0s 586us/step - loss: 0.0805 - val_loss: 0.0817\n",
      "Epoch 12/200\n",
      "420/420 [==============================] - 0s 590us/step - loss: 0.0804 - val_loss: 0.0797\n",
      "Epoch 13/200\n",
      "420/420 [==============================] - 0s 589us/step - loss: 0.0798 - val_loss: 0.0794\n",
      "Epoch 14/200\n",
      "420/420 [==============================] - 0s 582us/step - loss: 0.0796 - val_loss: 0.0808\n",
      "Epoch 15/200\n",
      "420/420 [==============================] - 0s 615us/step - loss: 0.0795 - val_loss: 0.0793\n",
      "Epoch 16/200\n",
      "420/420 [==============================] - 0s 581us/step - loss: 0.0790 - val_loss: 0.0786\n",
      "Epoch 17/200\n",
      "420/420 [==============================] - 0s 585us/step - loss: 0.0786 - val_loss: 0.0785\n",
      "Epoch 18/200\n",
      "420/420 [==============================] - 0s 587us/step - loss: 0.0783 - val_loss: 0.0783\n",
      "Epoch 19/200\n",
      "420/420 [==============================] - 0s 587us/step - loss: 0.0778 - val_loss: 0.0784\n",
      "Epoch 20/200\n",
      "420/420 [==============================] - 0s 590us/step - loss: 0.0776 - val_loss: 0.0784\n",
      "Epoch 21/200\n",
      "420/420 [==============================] - 0s 586us/step - loss: 0.0778 - val_loss: 0.0788\n",
      "Epoch 22/200\n",
      "420/420 [==============================] - 0s 588us/step - loss: 0.0776 - val_loss: 0.0805\n",
      "Epoch 23/200\n",
      "420/420 [==============================] - 0s 585us/step - loss: 0.0770 - val_loss: 0.0799\n",
      "Epoch 24/200\n",
      "420/420 [==============================] - 0s 588us/step - loss: 0.0770 - val_loss: 0.0775\n",
      "Epoch 25/200\n",
      "420/420 [==============================] - 0s 594us/step - loss: 0.0766 - val_loss: 0.0772\n",
      "Epoch 26/200\n",
      "420/420 [==============================] - 0s 584us/step - loss: 0.0763 - val_loss: 0.0774\n",
      "Epoch 27/200\n",
      "420/420 [==============================] - 0s 593us/step - loss: 0.0762 - val_loss: 0.0787\n",
      "Epoch 28/200\n",
      "420/420 [==============================] - 0s 591us/step - loss: 0.0761 - val_loss: 0.0771\n",
      "Epoch 29/200\n",
      "420/420 [==============================] - 0s 593us/step - loss: 0.0754 - val_loss: 0.0794\n",
      "Epoch 30/200\n",
      "420/420 [==============================] - 0s 590us/step - loss: 0.0755 - val_loss: 0.0767\n",
      "Epoch 31/200\n",
      "420/420 [==============================] - 0s 588us/step - loss: 0.0754 - val_loss: 0.0764\n",
      "Epoch 32/200\n",
      "420/420 [==============================] - 0s 590us/step - loss: 0.0751 - val_loss: 0.0775\n",
      "Epoch 33/200\n",
      "420/420 [==============================] - 0s 590us/step - loss: 0.0747 - val_loss: 0.0765\n",
      "Epoch 34/200\n",
      "420/420 [==============================] - 0s 586us/step - loss: 0.0746 - val_loss: 0.0761\n",
      "Epoch 35/200\n",
      "420/420 [==============================] - 0s 592us/step - loss: 0.0746 - val_loss: 0.0777\n",
      "Epoch 36/200\n",
      "420/420 [==============================] - 0s 596us/step - loss: 0.0745 - val_loss: 0.0760\n",
      "Epoch 37/200\n",
      "420/420 [==============================] - 0s 596us/step - loss: 0.0741 - val_loss: 0.0801\n",
      "Epoch 38/200\n",
      "420/420 [==============================] - 0s 587us/step - loss: 0.0738 - val_loss: 0.0770\n",
      "Epoch 39/200\n",
      "420/420 [==============================] - 0s 595us/step - loss: 0.0742 - val_loss: 0.0767\n",
      "Epoch 40/200\n",
      "420/420 [==============================] - 0s 592us/step - loss: 0.0738 - val_loss: 0.0752\n",
      "Epoch 41/200\n",
      "420/420 [==============================] - 0s 590us/step - loss: 0.0735 - val_loss: 0.0752\n",
      "Epoch 42/200\n",
      "420/420 [==============================] - 0s 595us/step - loss: 0.0731 - val_loss: 0.0753\n",
      "Epoch 43/200\n",
      "420/420 [==============================] - 0s 594us/step - loss: 0.0730 - val_loss: 0.0753\n",
      "Epoch 44/200\n",
      "420/420 [==============================] - 0s 616us/step - loss: 0.0727 - val_loss: 0.0782\n",
      "Epoch 45/200\n",
      "420/420 [==============================] - 0s 582us/step - loss: 0.0731 - val_loss: 0.0759\n",
      "Epoch 46/200\n",
      "420/420 [==============================] - 0s 587us/step - loss: 0.0727 - val_loss: 0.0755\n",
      "Epoch 47/200\n",
      "420/420 [==============================] - 0s 595us/step - loss: 0.0723 - val_loss: 0.0750\n",
      "Epoch 48/200\n",
      "420/420 [==============================] - 0s 624us/step - loss: 0.0729 - val_loss: 0.0753\n",
      "Epoch 49/200\n",
      "420/420 [==============================] - 0s 615us/step - loss: 0.0723 - val_loss: 0.0767\n",
      "Epoch 50/200\n",
      "420/420 [==============================] - 0s 585us/step - loss: 0.0726 - val_loss: 0.0747\n",
      "Epoch 51/200\n",
      "420/420 [==============================] - 0s 595us/step - loss: 0.0724 - val_loss: 0.0756\n",
      "Epoch 52/200\n",
      "420/420 [==============================] - 0s 591us/step - loss: 0.0724 - val_loss: 0.0741\n",
      "Epoch 53/200\n",
      "420/420 [==============================] - 0s 597us/step - loss: 0.0719 - val_loss: 0.0739\n",
      "Epoch 54/200\n",
      "420/420 [==============================] - 0s 617us/step - loss: 0.0716 - val_loss: 0.0748\n",
      "Epoch 55/200\n",
      "420/420 [==============================] - 0s 602us/step - loss: 0.0718 - val_loss: 0.0738\n",
      "Epoch 56/200\n",
      "420/420 [==============================] - 0s 585us/step - loss: 0.0715 - val_loss: 0.0740\n",
      "Epoch 57/200\n",
      "420/420 [==============================] - 0s 585us/step - loss: 0.0717 - val_loss: 0.0735\n",
      "Epoch 58/200\n",
      "420/420 [==============================] - 0s 585us/step - loss: 0.0711 - val_loss: 0.0738\n",
      "Epoch 59/200\n",
      "420/420 [==============================] - 0s 583us/step - loss: 0.0714 - val_loss: 0.0736\n",
      "Epoch 60/200\n",
      "420/420 [==============================] - 0s 589us/step - loss: 0.0714 - val_loss: 0.0745\n",
      "Epoch 61/200\n",
      "420/420 [==============================] - 0s 584us/step - loss: 0.0714 - val_loss: 0.0737\n",
      "Epoch 62/200\n",
      "420/420 [==============================] - 0s 580us/step - loss: 0.0708 - val_loss: 0.0732\n",
      "Epoch 63/200\n",
      "420/420 [==============================] - 0s 583us/step - loss: 0.0712 - val_loss: 0.0739\n",
      "Epoch 64/200\n",
      "420/420 [==============================] - 0s 588us/step - loss: 0.0707 - val_loss: 0.0732\n",
      "Epoch 65/200\n",
      "420/420 [==============================] - 0s 583us/step - loss: 0.0712 - val_loss: 0.0732\n",
      "Epoch 66/200\n",
      "420/420 [==============================] - 0s 584us/step - loss: 0.0710 - val_loss: 0.0753\n",
      "Epoch 67/200\n",
      "420/420 [==============================] - 0s 589us/step - loss: 0.0710 - val_loss: 0.0748\n",
      "Epoch 68/200\n",
      "420/420 [==============================] - 0s 586us/step - loss: 0.0705 - val_loss: 0.0735\n",
      "Epoch 69/200\n",
      "420/420 [==============================] - 0s 589us/step - loss: 0.0706 - val_loss: 0.0733\n",
      "Epoch 70/200\n",
      "420/420 [==============================] - 0s 600us/step - loss: 0.0702 - val_loss: 0.0738\n",
      "Epoch 71/200\n",
      "420/420 [==============================] - 0s 584us/step - loss: 0.0706 - val_loss: 0.0728\n",
      "Epoch 72/200\n",
      "420/420 [==============================] - 0s 584us/step - loss: 0.0702 - val_loss: 0.0730\n",
      "Epoch 73/200\n",
      "420/420 [==============================] - 0s 592us/step - loss: 0.0702 - val_loss: 0.0729\n",
      "Epoch 74/200\n",
      "420/420 [==============================] - 0s 589us/step - loss: 0.0696 - val_loss: 0.0728\n",
      "Epoch 75/200\n",
      "420/420 [==============================] - 0s 587us/step - loss: 0.0698 - val_loss: 0.0784\n",
      "Epoch 76/200\n",
      "420/420 [==============================] - 0s 589us/step - loss: 0.0696 - val_loss: 0.0750\n",
      "Epoch 77/200\n",
      "420/420 [==============================] - 0s 588us/step - loss: 0.0698 - val_loss: 0.0725\n",
      "Epoch 78/200\n",
      "420/420 [==============================] - 0s 585us/step - loss: 0.0693 - val_loss: 0.0779\n",
      "Epoch 79/200\n",
      "420/420 [==============================] - 0s 595us/step - loss: 0.0698 - val_loss: 0.0725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "420/420 [==============================] - 0s 585us/step - loss: 0.0699 - val_loss: 0.0738\n",
      "Epoch 81/200\n",
      "420/420 [==============================] - 0s 581us/step - loss: 0.0701 - val_loss: 0.0722\n",
      "Epoch 82/200\n",
      "420/420 [==============================] - 0s 575us/step - loss: 0.0700 - val_loss: 0.0723\n",
      "Epoch 83/200\n",
      "420/420 [==============================] - 0s 579us/step - loss: 0.0693 - val_loss: 0.0720\n",
      "Epoch 84/200\n",
      "420/420 [==============================] - 0s 582us/step - loss: 0.0695 - val_loss: 0.0727\n",
      "Epoch 85/200\n",
      "420/420 [==============================] - 0s 580us/step - loss: 0.0693 - val_loss: 0.0728\n",
      "Epoch 86/200\n",
      "420/420 [==============================] - 0s 576us/step - loss: 0.0691 - val_loss: 0.0719\n",
      "Epoch 87/200\n",
      "420/420 [==============================] - 0s 580us/step - loss: 0.0697 - val_loss: 0.0727\n",
      "Epoch 88/200\n",
      "420/420 [==============================] - 0s 581us/step - loss: 0.0689 - val_loss: 0.0759\n",
      "Epoch 89/200\n",
      "420/420 [==============================] - 0s 579us/step - loss: 0.0692 - val_loss: 0.0752\n",
      "Epoch 90/200\n",
      "420/420 [==============================] - 0s 585us/step - loss: 0.0698 - val_loss: 0.0719\n",
      "Epoch 91/200\n",
      "420/420 [==============================] - 0s 579us/step - loss: 0.0696 - val_loss: 0.0723\n",
      "Epoch 92/200\n",
      "420/420 [==============================] - 0s 583us/step - loss: 0.0692 - val_loss: 0.0735\n",
      "Epoch 93/200\n",
      "420/420 [==============================] - 0s 583us/step - loss: 0.0688 - val_loss: 0.0735\n",
      "Epoch 94/200\n",
      "420/420 [==============================] - 0s 582us/step - loss: 0.0690 - val_loss: 0.0721\n",
      "Epoch 95/200\n",
      "420/420 [==============================] - 0s 596us/step - loss: 0.0682 - val_loss: 0.0736\n",
      "Epoch 96/200\n",
      "420/420 [==============================] - 0s 576us/step - loss: 0.0690 - val_loss: 0.0719\n",
      "Epoch 97/200\n",
      "420/420 [==============================] - 0s 587us/step - loss: 0.0693 - val_loss: 0.0747\n",
      "Epoch 98/200\n",
      "420/420 [==============================] - 0s 583us/step - loss: 0.0688 - val_loss: 0.0716\n",
      "Epoch 99/200\n",
      "420/420 [==============================] - 0s 578us/step - loss: 0.0686 - val_loss: 0.0717\n",
      "Epoch 100/200\n",
      "420/420 [==============================] - 0s 578us/step - loss: 0.0687 - val_loss: 0.0713\n",
      "Epoch 101/200\n",
      "420/420 [==============================] - 0s 583us/step - loss: 0.0688 - val_loss: 0.0723\n",
      "Epoch 102/200\n",
      "420/420 [==============================] - 0s 581us/step - loss: 0.0686 - val_loss: 0.0715\n",
      "Epoch 103/200\n",
      "420/420 [==============================] - 0s 582us/step - loss: 0.0684 - val_loss: 0.0732\n",
      "Epoch 104/200\n",
      "420/420 [==============================] - 0s 587us/step - loss: 0.0685 - val_loss: 0.0712\n",
      "Epoch 105/200\n",
      "420/420 [==============================] - 0s 582us/step - loss: 0.0685 - val_loss: 0.0717\n",
      "Epoch 106/200\n",
      "420/420 [==============================] - 0s 580us/step - loss: 0.0682 - val_loss: 0.0713\n",
      "Epoch 107/200\n",
      "420/420 [==============================] - 0s 586us/step - loss: 0.0683 - val_loss: 0.0712\n",
      "Epoch 108/200\n",
      "420/420 [==============================] - 0s 576us/step - loss: 0.0688 - val_loss: 0.0710\n",
      "Epoch 109/200\n",
      "420/420 [==============================] - 0s 579us/step - loss: 0.0685 - val_loss: 0.0713\n",
      "Epoch 110/200\n",
      "420/420 [==============================] - 0s 581us/step - loss: 0.0687 - val_loss: 0.0713\n",
      "Epoch 111/200\n",
      "420/420 [==============================] - 0s 579us/step - loss: 0.0688 - val_loss: 0.0722\n",
      "Epoch 112/200\n",
      "420/420 [==============================] - 0s 598us/step - loss: 0.0682 - val_loss: 0.0715\n",
      "Epoch 113/200\n",
      "420/420 [==============================] - 0s 587us/step - loss: 0.0686 - val_loss: 0.0709\n",
      "Epoch 114/200\n",
      "420/420 [==============================] - 0s 583us/step - loss: 0.0682 - val_loss: 0.0728\n",
      "Epoch 115/200\n",
      "420/420 [==============================] - 0s 585us/step - loss: 0.0684 - val_loss: 0.0709\n",
      "Epoch 116/200\n",
      "420/420 [==============================] - 0s 586us/step - loss: 0.0684 - val_loss: 0.0735\n",
      "Epoch 117/200\n",
      "420/420 [==============================] - 0s 581us/step - loss: 0.0683 - val_loss: 0.0725\n",
      "Epoch 118/200\n",
      "420/420 [==============================] - 0s 579us/step - loss: 0.0688 - val_loss: 0.0712\n",
      "Epoch 119/200\n",
      "420/420 [==============================] - 0s 580us/step - loss: 0.0687 - val_loss: 0.0727\n",
      "Epoch 120/200\n",
      "420/420 [==============================] - 0s 595us/step - loss: 0.0678 - val_loss: 0.0709\n",
      "Epoch 121/200\n",
      "420/420 [==============================] - 0s 587us/step - loss: 0.0680 - val_loss: 0.0735\n",
      "Epoch 122/200\n",
      "420/420 [==============================] - 0s 580us/step - loss: 0.0685 - val_loss: 0.0713\n",
      "Epoch 123/200\n",
      "420/420 [==============================] - 0s 583us/step - loss: 0.0681 - val_loss: 0.0744\n",
      "Epoch 124/200\n",
      "420/420 [==============================] - 0s 583us/step - loss: 0.0680 - val_loss: 0.0709\n",
      "Epoch 125/200\n",
      "420/420 [==============================] - 0s 588us/step - loss: 0.0683 - val_loss: 0.0713\n",
      "Epoch 126/200\n",
      "420/420 [==============================] - 0s 582us/step - loss: 0.0682 - val_loss: 0.0716\n",
      "Epoch 127/200\n",
      "420/420 [==============================] - 0s 584us/step - loss: 0.0678 - val_loss: 0.0835\n",
      "Epoch 128/200\n",
      "420/420 [==============================] - 0s 586us/step - loss: 0.0682 - val_loss: 0.0721\n",
      "Epoch 129/200\n",
      "420/420 [==============================] - 0s 580us/step - loss: 0.0686 - val_loss: 0.0712\n",
      "Epoch 130/200\n",
      "420/420 [==============================] - 0s 581us/step - loss: 0.0679 - val_loss: 0.0712\n",
      "Epoch 131/200\n",
      "420/420 [==============================] - 0s 581us/step - loss: 0.0681 - val_loss: 0.0753\n",
      "Epoch 132/200\n",
      "420/420 [==============================] - 0s 586us/step - loss: 0.0681 - val_loss: 0.0708\n",
      "Epoch 133/200\n",
      "420/420 [==============================] - 0s 578us/step - loss: 0.0677 - val_loss: 0.0707\n",
      "Epoch 134/200\n",
      "420/420 [==============================] - 0s 587us/step - loss: 0.0683 - val_loss: 0.0714\n",
      "Epoch 135/200\n",
      "420/420 [==============================] - 0s 586us/step - loss: 0.0682 - val_loss: 0.0722\n",
      "Epoch 136/200\n",
      "420/420 [==============================] - 0s 594us/step - loss: 0.0683 - val_loss: 0.0708\n",
      "Epoch 137/200\n",
      "420/420 [==============================] - 0s 591us/step - loss: 0.0681 - val_loss: 0.0706\n",
      "Epoch 138/200\n",
      "420/420 [==============================] - 0s 578us/step - loss: 0.0681 - val_loss: 0.0712\n",
      "Epoch 139/200\n",
      "420/420 [==============================] - 0s 584us/step - loss: 0.0677 - val_loss: 0.0715\n",
      "Epoch 140/200\n",
      "420/420 [==============================] - 0s 586us/step - loss: 0.0681 - val_loss: 0.0708\n",
      "Epoch 141/200\n",
      "420/420 [==============================] - 0s 579us/step - loss: 0.0680 - val_loss: 0.0707\n",
      "Epoch 142/200\n",
      "420/420 [==============================] - 0s 583us/step - loss: 0.0676 - val_loss: 0.0715\n",
      "Epoch 143/200\n",
      "420/420 [==============================] - 0s 582us/step - loss: 0.0678 - val_loss: 0.0717\n",
      "Epoch 144/200\n",
      "420/420 [==============================] - 0s 580us/step - loss: 0.0681 - val_loss: 0.0710\n",
      "Epoch 145/200\n",
      "420/420 [==============================] - 0s 584us/step - loss: 0.0681 - val_loss: 0.0705\n",
      "Epoch 146/200\n",
      "420/420 [==============================] - 0s 581us/step - loss: 0.0679 - val_loss: 0.0710\n",
      "Epoch 147/200\n",
      "420/420 [==============================] - 0s 590us/step - loss: 0.0677 - val_loss: 0.0710\n",
      "Epoch 148/200\n",
      "420/420 [==============================] - 0s 588us/step - loss: 0.0679 - val_loss: 0.0708\n",
      "Epoch 149/200\n",
      "420/420 [==============================] - 0s 584us/step - loss: 0.0681 - val_loss: 0.0703\n",
      "Epoch 150/200\n",
      "420/420 [==============================] - 0s 581us/step - loss: 0.0676 - val_loss: 0.0713\n",
      "Epoch 151/200\n",
      "420/420 [==============================] - 0s 587us/step - loss: 0.0678 - val_loss: 0.0706\n",
      "Epoch 152/200\n",
      "420/420 [==============================] - 0s 591us/step - loss: 0.0677 - val_loss: 0.0707\n",
      "Epoch 153/200\n",
      "420/420 [==============================] - 0s 603us/step - loss: 0.0680 - val_loss: 0.0703\n",
      "Epoch 154/200\n",
      "420/420 [==============================] - 0s 592us/step - loss: 0.0678 - val_loss: 0.0715\n",
      "Epoch 155/200\n",
      "420/420 [==============================] - 0s 607us/step - loss: 0.0674 - val_loss: 0.0723\n",
      "Epoch 156/200\n",
      "420/420 [==============================] - 0s 600us/step - loss: 0.0677 - val_loss: 0.0703\n",
      "Epoch 157/200\n",
      "420/420 [==============================] - 0s 627us/step - loss: 0.0680 - val_loss: 0.0706\n",
      "Epoch 158/200\n",
      "420/420 [==============================] - 0s 619us/step - loss: 0.0677 - val_loss: 0.0702\n",
      "Epoch 159/200\n",
      "420/420 [==============================] - 0s 603us/step - loss: 0.0676 - val_loss: 0.0709\n",
      "Epoch 160/200\n",
      "420/420 [==============================] - 0s 591us/step - loss: 0.0677 - val_loss: 0.0707\n",
      "Epoch 161/200\n",
      "420/420 [==============================] - 0s 600us/step - loss: 0.0677 - val_loss: 0.0706\n",
      "Epoch 162/200\n",
      "420/420 [==============================] - 0s 584us/step - loss: 0.0678 - val_loss: 0.0722\n",
      "Epoch 163/200\n",
      "420/420 [==============================] - 0s 580us/step - loss: 0.0678 - val_loss: 0.0705\n",
      "Epoch 164/200\n",
      "420/420 [==============================] - 0s 578us/step - loss: 0.0680 - val_loss: 0.0749\n",
      "Epoch 165/200\n",
      "420/420 [==============================] - 0s 584us/step - loss: 0.0681 - val_loss: 0.0710\n",
      "Epoch 166/200\n",
      "420/420 [==============================] - 0s 581us/step - loss: 0.0677 - val_loss: 0.0709\n",
      "Epoch 167/200\n",
      "420/420 [==============================] - 0s 579us/step - loss: 0.0683 - val_loss: 0.0704\n",
      "Epoch 168/200\n",
      "420/420 [==============================] - 0s 586us/step - loss: 0.0674 - val_loss: 0.0705\n",
      "Epoch 169/200\n",
      "420/420 [==============================] - 0s 577us/step - loss: 0.0680 - val_loss: 0.0708\n",
      "Epoch 170/200\n",
      "420/420 [==============================] - 0s 583us/step - loss: 0.0675 - val_loss: 0.0705\n",
      "Epoch 171/200\n",
      "420/420 [==============================] - 0s 575us/step - loss: 0.0677 - val_loss: 0.0707\n",
      "Epoch 172/200\n",
      "420/420 [==============================] - 0s 579us/step - loss: 0.0677 - val_loss: 0.0709\n",
      "Epoch 173/200\n",
      "420/420 [==============================] - 0s 586us/step - loss: 0.0675 - val_loss: 0.0707\n",
      "Epoch 174/200\n",
      "420/420 [==============================] - 0s 585us/step - loss: 0.0676 - val_loss: 0.0720\n",
      "Epoch 175/200\n",
      "420/420 [==============================] - 0s 585us/step - loss: 0.0681 - val_loss: 0.0711\n",
      "Epoch 176/200\n",
      "420/420 [==============================] - 0s 588us/step - loss: 0.0672 - val_loss: 0.0711\n",
      "Epoch 177/200\n",
      "420/420 [==============================] - 0s 597us/step - loss: 0.0676 - val_loss: 0.0711\n",
      "Epoch 178/200\n",
      "420/420 [==============================] - 0s 599us/step - loss: 0.0682 - val_loss: 0.0701\n",
      "Epoch 179/200\n",
      "420/420 [==============================] - 0s 583us/step - loss: 0.0676 - val_loss: 0.0707\n",
      "Epoch 180/200\n",
      "420/420 [==============================] - 0s 581us/step - loss: 0.0679 - val_loss: 0.0709\n",
      "Epoch 181/200\n",
      "420/420 [==============================] - 0s 588us/step - loss: 0.0675 - val_loss: 0.0738\n",
      "Epoch 182/200\n",
      "420/420 [==============================] - 0s 597us/step - loss: 0.0671 - val_loss: 0.0712\n",
      "Epoch 183/200\n",
      "420/420 [==============================] - 0s 587us/step - loss: 0.0680 - val_loss: 0.0723\n",
      "Epoch 184/200\n",
      "420/420 [==============================] - 0s 588us/step - loss: 0.0675 - val_loss: 0.0706\n",
      "Epoch 185/200\n",
      "420/420 [==============================] - 0s 587us/step - loss: 0.0678 - val_loss: 0.0704\n",
      "Epoch 186/200\n",
      "420/420 [==============================] - 0s 584us/step - loss: 0.0680 - val_loss: 0.0732\n",
      "Epoch 187/200\n",
      "420/420 [==============================] - 0s 585us/step - loss: 0.0680 - val_loss: 0.0710\n",
      "Epoch 188/200\n",
      "420/420 [==============================] - 0s 583us/step - loss: 0.0679 - val_loss: 0.0703\n",
      "Epoch 189/200\n",
      "420/420 [==============================] - 0s 585us/step - loss: 0.0674 - val_loss: 0.0707\n",
      "Epoch 190/200\n",
      "420/420 [==============================] - 0s 583us/step - loss: 0.0677 - val_loss: 0.0750\n",
      "Epoch 191/200\n",
      "420/420 [==============================] - 0s 591us/step - loss: 0.0678 - val_loss: 0.0703\n",
      "Epoch 192/200\n",
      "420/420 [==============================] - 0s 581us/step - loss: 0.0676 - val_loss: 0.0703\n",
      "Epoch 193/200\n",
      "420/420 [==============================] - 0s 586us/step - loss: 0.0676 - val_loss: 0.0728\n",
      "Epoch 194/200\n",
      "420/420 [==============================] - 0s 588us/step - loss: 0.0676 - val_loss: 0.0717\n",
      "Epoch 195/200\n",
      "420/420 [==============================] - 0s 582us/step - loss: 0.0679 - val_loss: 0.0718\n",
      "Epoch 196/200\n",
      "420/420 [==============================] - 0s 591us/step - loss: 0.0675 - val_loss: 0.0712\n",
      "Epoch 197/200\n",
      "420/420 [==============================] - 0s 593us/step - loss: 0.0676 - val_loss: 0.0704\n",
      "Epoch 198/200\n",
      "420/420 [==============================] - 0s 588us/step - loss: 0.0684 - val_loss: 0.0706\n",
      "Epoch 199/200\n",
      "420/420 [==============================] - 0s 582us/step - loss: 0.0676 - val_loss: 0.0707\n",
      "Epoch 200/200\n",
      "420/420 [==============================] - 0s 588us/step - loss: 0.0676 - val_loss: 0.0722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe8f7f80460>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, epochs=200, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe8fc57ea00>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "losses.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our validation loss remains relatively stable with the actual loss, so overfitting is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = np.argmax(model.predict(X_test),axis=-1)\n",
    "y_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX:\n",
      "\n",
      "4034\t36\n",
      "56\t349\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_test,y_pred)\n",
    "print(f'CONFUSION MATRIX:\\n\\n{confusion[0][0]}\\t{confusion[0][1]}\\n{confusion[1][0]}\\t{confusion[1][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION REPORT:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4070\n",
      "           1       0.91      0.86      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.95      0.93      0.94      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"CLASSIFICATION REPORT:\\n\\n{classification_report(y_test,y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating Through the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's experiment with the number of hidden layers in our network. We'll iterate from 2 to 50 hidden layers, each containing 8 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION REPORT FOR 2 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      4070\n",
      "           1       0.93      0.83      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.96      0.91      0.93      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 3 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      4070\n",
      "           1       0.94      0.83      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.96      0.91      0.93      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 4 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4070\n",
      "           1       0.91      0.86      0.89       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.95      0.93      0.94      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 5 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      4070\n",
      "           1       0.92      0.84      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.95      0.92      0.93      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 6 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      4070\n",
      "           1       0.93      0.84      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.96      0.92      0.93      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 7 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      4070\n",
      "           1       0.94      0.82      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.96      0.91      0.93      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 8 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      4070\n",
      "           1       0.92      0.84      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.95      0.92      0.93      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 9 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4070\n",
      "           1       0.91      0.86      0.89       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.95      0.93      0.94      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 10 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      4070\n",
      "           1       0.93      0.84      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.96      0.92      0.94      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 11 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4070\n",
      "           1       0.91      0.86      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.95      0.93      0.94      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 12 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      4070\n",
      "           1       0.94      0.82      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.96      0.91      0.93      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 13 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      4070\n",
      "           1       0.94      0.83      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.96      0.91      0.94      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 14 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      4070\n",
      "           1       0.95      0.82      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.96      0.91      0.94      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 15 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      4070\n",
      "           1       0.93      0.82      0.87       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.96      0.91      0.93      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 16 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4070\n",
      "           1       0.91      0.85      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.95      0.92      0.93      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 17 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      4070\n",
      "           1       0.93      0.83      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.96      0.91      0.93      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 18 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      4070\n",
      "           1       0.95      0.82      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.96      0.91      0.93      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-145-31c4091dd8c8>:34: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = tp/(tp+fp)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION REPORT FOR 19 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 20 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 21 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 22 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 23 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 24 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4070\n",
      "           1       0.92      0.86      0.89       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.95      0.92      0.94      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 25 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      4070\n",
      "           1       0.92      0.85      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.95      0.92      0.94      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 26 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      4070\n",
      "           1       0.93      0.84      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.96      0.92      0.94      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 27 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4070\n",
      "           1       0.91      0.85      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.95      0.92      0.93      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 28 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 29 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 30 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4070\n",
      "           1       0.92      0.85      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.95      0.92      0.94      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 31 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4070\n",
      "           1       0.90      0.87      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.94      0.93      0.94      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 32 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      4070\n",
      "           1       0.95      0.83      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.96      0.91      0.94      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 33 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      4070\n",
      "           1       0.94      0.81      0.87       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.96      0.90      0.93      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 34 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 35 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      4070\n",
      "           1       0.96      0.80      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.97      0.90      0.93      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 36 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 37 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      4070\n",
      "           1       0.92      0.85      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.95      0.92      0.94      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 38 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4070\n",
      "           1       0.93      0.85      0.89       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.96      0.92      0.94      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 39 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 40 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 41 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION REPORT FOR 42 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      4070\n",
      "           1       0.92      0.85      0.88       405\n",
      "\n",
      "    accuracy                           0.98      4475\n",
      "   macro avg       0.95      0.92      0.94      4475\n",
      "weighted avg       0.98      0.98      0.98      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 43 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 44 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 45 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 46 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 47 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 48 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 49 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n",
      "CLASSIFICATION REPORT FOR 50 HIDDEN LAYERS:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95      4070\n",
      "           1       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.91      4475\n",
      "   macro avg       0.45      0.50      0.48      4475\n",
      "weighted avg       0.83      0.91      0.87      4475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(2,51):\n",
    "    model_loop = Sequential()\n",
    "    \n",
    "    # Input and hidden layers\n",
    "    for j in range(0,(i+1)):\n",
    "        model_loop.add(Dense(8,activation='relu'))\n",
    "    \n",
    "    # Output layer\n",
    "    model_loop.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "    # Compile layers\n",
    "    model_loop.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    \n",
    "    # We will reduce epochs to 100 to reduce run time. \n",
    "    # 100 was chosen based on previous loss function visualization.\n",
    "    model_loop.fit(x=X_train,y=y_train.values,\n",
    "          validation_data=(X_test,y_test.values),\n",
    "          batch_size=128,epochs=100,verbose=0)\n",
    "    \n",
    "    # Model evaluation\n",
    "    predictions_loop = model_loop.predict_classes(X_test)\n",
    "    \n",
    "    # Calculate statistics for each iteration\n",
    "    confusion = confusion_matrix(y_test,predictions_loop)\n",
    "    \n",
    "    tp = confusion[1][1]\n",
    "    tn = confusion[0][0]\n",
    "    fp = confusion[0][1]\n",
    "    fn = confusion[1][0]\n",
    "    \n",
    "    total = tp+tn+fp+fn\n",
    "    accuracy = (tp+tn)/total\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1_score = 2*precision*recall/(precision+recall)\n",
    "    \n",
    "    # Append results to the list for future reference\n",
    "    results.append([i,accuracy,precision,recall,f1_score])\n",
    "    \n",
    "    # Print classification report after each iteration\n",
    "    print(f\"CLASSIFICATION REPORT FOR {i} HIDDEN LAYERS:\\n\\n{classification_report(y_test,predictions_loop)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the results in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hidden_Layers</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.978771</td>\n",
       "      <td>0.928177</td>\n",
       "      <td>0.829630</td>\n",
       "      <td>0.876141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.979665</td>\n",
       "      <td>0.936111</td>\n",
       "      <td>0.832099</td>\n",
       "      <td>0.881046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.979888</td>\n",
       "      <td>0.913386</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.885496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.979218</td>\n",
       "      <td>0.921622</td>\n",
       "      <td>0.841975</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.979218</td>\n",
       "      <td>0.926230</td>\n",
       "      <td>0.837037</td>\n",
       "      <td>0.879377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.979218</td>\n",
       "      <td>0.943182</td>\n",
       "      <td>0.819753</td>\n",
       "      <td>0.877147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.978994</td>\n",
       "      <td>0.921409</td>\n",
       "      <td>0.839506</td>\n",
       "      <td>0.878553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.980112</td>\n",
       "      <td>0.911458</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.887199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.979888</td>\n",
       "      <td>0.929155</td>\n",
       "      <td>0.841975</td>\n",
       "      <td>0.883420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>0.979218</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.882129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>0.979218</td>\n",
       "      <td>0.940678</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.877470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>0.979888</td>\n",
       "      <td>0.936288</td>\n",
       "      <td>0.834568</td>\n",
       "      <td>0.882507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>0.979888</td>\n",
       "      <td>0.946176</td>\n",
       "      <td>0.824691</td>\n",
       "      <td>0.881266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>0.978547</td>\n",
       "      <td>0.930362</td>\n",
       "      <td>0.824691</td>\n",
       "      <td>0.874346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>0.978994</td>\n",
       "      <td>0.912467</td>\n",
       "      <td>0.849383</td>\n",
       "      <td>0.879795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>0.978771</td>\n",
       "      <td>0.928177</td>\n",
       "      <td>0.829630</td>\n",
       "      <td>0.876141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>0.979665</td>\n",
       "      <td>0.946023</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.879789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>22</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24</td>\n",
       "      <td>0.980112</td>\n",
       "      <td>0.917989</td>\n",
       "      <td>0.856790</td>\n",
       "      <td>0.886335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25</td>\n",
       "      <td>0.979441</td>\n",
       "      <td>0.919571</td>\n",
       "      <td>0.846914</td>\n",
       "      <td>0.881748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>26</td>\n",
       "      <td>0.979665</td>\n",
       "      <td>0.931319</td>\n",
       "      <td>0.837037</td>\n",
       "      <td>0.881664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>27</td>\n",
       "      <td>0.979218</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.849383</td>\n",
       "      <td>0.880922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>28</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>0.979665</td>\n",
       "      <td>0.917553</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.883483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>0.979441</td>\n",
       "      <td>0.900256</td>\n",
       "      <td>0.869136</td>\n",
       "      <td>0.884422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>0.980335</td>\n",
       "      <td>0.946479</td>\n",
       "      <td>0.829630</td>\n",
       "      <td>0.884211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>0.978324</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.871863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>0.979218</td>\n",
       "      <td>0.958824</td>\n",
       "      <td>0.804938</td>\n",
       "      <td>0.875168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>0.979888</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.846914</td>\n",
       "      <td>0.884021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>0.981006</td>\n",
       "      <td>0.932432</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.890323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>0.979441</td>\n",
       "      <td>0.919571</td>\n",
       "      <td>0.846914</td>\n",
       "      <td>0.881748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>44</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>45</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>46</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>47</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>48</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>49</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>50</td>\n",
       "      <td>0.909497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Hidden_Layers  Accuracy  Precision    Recall  F1-Score\n",
       "0               2  0.978771   0.928177  0.829630  0.876141\n",
       "1               3  0.979665   0.936111  0.832099  0.881046\n",
       "2               4  0.979888   0.913386  0.859259  0.885496\n",
       "3               5  0.979218   0.921622  0.841975  0.880000\n",
       "4               6  0.979218   0.926230  0.837037  0.879377\n",
       "5               7  0.979218   0.943182  0.819753  0.877147\n",
       "6               8  0.978994   0.921409  0.839506  0.878553\n",
       "7               9  0.980112   0.911458  0.864198  0.887199\n",
       "8              10  0.979888   0.929155  0.841975  0.883420\n",
       "9              11  0.979218   0.906250  0.859259  0.882129\n",
       "10             12  0.979218   0.940678  0.822222  0.877470\n",
       "11             13  0.979888   0.936288  0.834568  0.882507\n",
       "12             14  0.979888   0.946176  0.824691  0.881266\n",
       "13             15  0.978547   0.930362  0.824691  0.874346\n",
       "14             16  0.978994   0.912467  0.849383  0.879795\n",
       "15             17  0.978771   0.928177  0.829630  0.876141\n",
       "16             18  0.979665   0.946023  0.822222  0.879789\n",
       "17             19  0.909497        NaN  0.000000       NaN\n",
       "18             20  0.909497        NaN  0.000000       NaN\n",
       "19             21  0.909497        NaN  0.000000       NaN\n",
       "20             22  0.909497        NaN  0.000000       NaN\n",
       "21             23  0.909497        NaN  0.000000       NaN\n",
       "22             24  0.980112   0.917989  0.856790  0.886335\n",
       "23             25  0.979441   0.919571  0.846914  0.881748\n",
       "24             26  0.979665   0.931319  0.837037  0.881664\n",
       "25             27  0.979218   0.914894  0.849383  0.880922\n",
       "26             28  0.909497        NaN  0.000000       NaN\n",
       "27             29  0.909497        NaN  0.000000       NaN\n",
       "28             30  0.979665   0.917553  0.851852  0.883483\n",
       "29             31  0.979441   0.900256  0.869136  0.884422\n",
       "30             32  0.980335   0.946479  0.829630  0.884211\n",
       "31             33  0.978324   0.937500  0.814815  0.871863\n",
       "32             34  0.909497        NaN  0.000000       NaN\n",
       "33             35  0.979218   0.958824  0.804938  0.875168\n",
       "34             36  0.909497        NaN  0.000000       NaN\n",
       "35             37  0.979888   0.924528  0.846914  0.884021\n",
       "36             38  0.981006   0.932432  0.851852  0.890323\n",
       "37             39  0.909497        NaN  0.000000       NaN\n",
       "38             40  0.909497        NaN  0.000000       NaN\n",
       "39             41  0.909497        NaN  0.000000       NaN\n",
       "40             42  0.979441   0.919571  0.846914  0.881748\n",
       "41             43  0.909497        NaN  0.000000       NaN\n",
       "42             44  0.909497        NaN  0.000000       NaN\n",
       "43             45  0.909497        NaN  0.000000       NaN\n",
       "44             46  0.909497        NaN  0.000000       NaN\n",
       "45             47  0.909497        NaN  0.000000       NaN\n",
       "46             48  0.909497        NaN  0.000000       NaN\n",
       "47             49  0.909497        NaN  0.000000       NaN\n",
       "48             50  0.909497        NaN  0.000000       NaN"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(columns=['Hidden_Layers','Accuracy','Precision','Recall','F1-Score'],data=results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be many null values. Let's remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hidden_Layers</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.978771</td>\n",
       "      <td>0.928177</td>\n",
       "      <td>0.829630</td>\n",
       "      <td>0.876141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.979665</td>\n",
       "      <td>0.936111</td>\n",
       "      <td>0.832099</td>\n",
       "      <td>0.881046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.979888</td>\n",
       "      <td>0.913386</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.885496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.979218</td>\n",
       "      <td>0.921622</td>\n",
       "      <td>0.841975</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.979218</td>\n",
       "      <td>0.926230</td>\n",
       "      <td>0.837037</td>\n",
       "      <td>0.879377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.979218</td>\n",
       "      <td>0.943182</td>\n",
       "      <td>0.819753</td>\n",
       "      <td>0.877147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.978994</td>\n",
       "      <td>0.921409</td>\n",
       "      <td>0.839506</td>\n",
       "      <td>0.878553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.980112</td>\n",
       "      <td>0.911458</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.887199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.979888</td>\n",
       "      <td>0.929155</td>\n",
       "      <td>0.841975</td>\n",
       "      <td>0.883420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>0.979218</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.859259</td>\n",
       "      <td>0.882129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>0.979218</td>\n",
       "      <td>0.940678</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.877470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>0.979888</td>\n",
       "      <td>0.936288</td>\n",
       "      <td>0.834568</td>\n",
       "      <td>0.882507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>0.979888</td>\n",
       "      <td>0.946176</td>\n",
       "      <td>0.824691</td>\n",
       "      <td>0.881266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>0.978547</td>\n",
       "      <td>0.930362</td>\n",
       "      <td>0.824691</td>\n",
       "      <td>0.874346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>0.978994</td>\n",
       "      <td>0.912467</td>\n",
       "      <td>0.849383</td>\n",
       "      <td>0.879795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>0.978771</td>\n",
       "      <td>0.928177</td>\n",
       "      <td>0.829630</td>\n",
       "      <td>0.876141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>0.979665</td>\n",
       "      <td>0.946023</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.879789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24</td>\n",
       "      <td>0.980112</td>\n",
       "      <td>0.917989</td>\n",
       "      <td>0.856790</td>\n",
       "      <td>0.886335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25</td>\n",
       "      <td>0.979441</td>\n",
       "      <td>0.919571</td>\n",
       "      <td>0.846914</td>\n",
       "      <td>0.881748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>26</td>\n",
       "      <td>0.979665</td>\n",
       "      <td>0.931319</td>\n",
       "      <td>0.837037</td>\n",
       "      <td>0.881664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>27</td>\n",
       "      <td>0.979218</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.849383</td>\n",
       "      <td>0.880922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>0.979665</td>\n",
       "      <td>0.917553</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.883483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>0.979441</td>\n",
       "      <td>0.900256</td>\n",
       "      <td>0.869136</td>\n",
       "      <td>0.884422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>0.980335</td>\n",
       "      <td>0.946479</td>\n",
       "      <td>0.829630</td>\n",
       "      <td>0.884211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>0.978324</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.871863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>0.979218</td>\n",
       "      <td>0.958824</td>\n",
       "      <td>0.804938</td>\n",
       "      <td>0.875168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>0.979888</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.846914</td>\n",
       "      <td>0.884021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>0.981006</td>\n",
       "      <td>0.932432</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.890323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>0.979441</td>\n",
       "      <td>0.919571</td>\n",
       "      <td>0.846914</td>\n",
       "      <td>0.881748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Hidden_Layers  Accuracy  Precision    Recall  F1-Score\n",
       "0               2  0.978771   0.928177  0.829630  0.876141\n",
       "1               3  0.979665   0.936111  0.832099  0.881046\n",
       "2               4  0.979888   0.913386  0.859259  0.885496\n",
       "3               5  0.979218   0.921622  0.841975  0.880000\n",
       "4               6  0.979218   0.926230  0.837037  0.879377\n",
       "5               7  0.979218   0.943182  0.819753  0.877147\n",
       "6               8  0.978994   0.921409  0.839506  0.878553\n",
       "7               9  0.980112   0.911458  0.864198  0.887199\n",
       "8              10  0.979888   0.929155  0.841975  0.883420\n",
       "9              11  0.979218   0.906250  0.859259  0.882129\n",
       "10             12  0.979218   0.940678  0.822222  0.877470\n",
       "11             13  0.979888   0.936288  0.834568  0.882507\n",
       "12             14  0.979888   0.946176  0.824691  0.881266\n",
       "13             15  0.978547   0.930362  0.824691  0.874346\n",
       "14             16  0.978994   0.912467  0.849383  0.879795\n",
       "15             17  0.978771   0.928177  0.829630  0.876141\n",
       "16             18  0.979665   0.946023  0.822222  0.879789\n",
       "22             24  0.980112   0.917989  0.856790  0.886335\n",
       "23             25  0.979441   0.919571  0.846914  0.881748\n",
       "24             26  0.979665   0.931319  0.837037  0.881664\n",
       "25             27  0.979218   0.914894  0.849383  0.880922\n",
       "28             30  0.979665   0.917553  0.851852  0.883483\n",
       "29             31  0.979441   0.900256  0.869136  0.884422\n",
       "30             32  0.980335   0.946479  0.829630  0.884211\n",
       "31             33  0.978324   0.937500  0.814815  0.871863\n",
       "33             35  0.979218   0.958824  0.804938  0.875168\n",
       "35             37  0.979888   0.924528  0.846914  0.884021\n",
       "36             38  0.981006   0.932432  0.851852  0.890323\n",
       "40             42  0.979441   0.919571  0.846914  0.881748"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_results = results_df.dropna()\n",
    "cleaned_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the performance by number of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fe8e9781340>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing Accuracy\n",
    "plt.scatter(x=cleaned_results['Hidden_Layers'],y=cleaned_results['Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fe8e0483760>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing Precision\n",
    "plt.scatter(x=cleaned_results['Hidden_Layers'],y=cleaned_results['Precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fe8fe99a130>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing Recall\n",
    "plt.scatter(x=cleaned_results['Hidden_Layers'],y=cleaned_results['Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fe8fe936850>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing F1-Score\n",
    "plt.scatter(x=cleaned_results['Hidden_Layers'],y=cleaned_results['F1-Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hidden_Layers</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18.965517</td>\n",
       "      <td>0.979480</td>\n",
       "      <td>0.927520</td>\n",
       "      <td>0.839251</td>\n",
       "      <td>0.880956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.040060</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.013611</td>\n",
       "      <td>0.015649</td>\n",
       "      <td>0.004102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.978324</td>\n",
       "      <td>0.900256</td>\n",
       "      <td>0.804938</td>\n",
       "      <td>0.871863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.979218</td>\n",
       "      <td>0.917989</td>\n",
       "      <td>0.829630</td>\n",
       "      <td>0.878553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.979441</td>\n",
       "      <td>0.928177</td>\n",
       "      <td>0.839506</td>\n",
       "      <td>0.881266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.979888</td>\n",
       "      <td>0.936288</td>\n",
       "      <td>0.849383</td>\n",
       "      <td>0.883483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.981006</td>\n",
       "      <td>0.958824</td>\n",
       "      <td>0.869136</td>\n",
       "      <td>0.890323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Hidden_Layers   Accuracy  Precision     Recall   F1-Score\n",
       "count      29.000000  29.000000  29.000000  29.000000  29.000000\n",
       "mean       18.965517   0.979480   0.927520   0.839251   0.880956\n",
       "std        12.040060   0.000567   0.013611   0.015649   0.004102\n",
       "min         2.000000   0.978324   0.900256   0.804938   0.871863\n",
       "25%         9.000000   0.979218   0.917989   0.829630   0.878553\n",
       "50%        16.000000   0.979441   0.928177   0.839506   0.881266\n",
       "75%        30.000000   0.979888   0.936288   0.849383   0.883483\n",
       "max        42.000000   0.981006   0.958824   0.869136   0.890323"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hidden_Layers</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>0.981006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>0.980335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.980112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24</td>\n",
       "      <td>0.980112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>0.979888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Hidden_Layers  Accuracy\n",
       "36             38  0.981006\n",
       "30             32  0.980335\n",
       "7               9  0.980112\n",
       "22             24  0.980112\n",
       "11             13  0.979888"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Top 5 Models by Accuracy\n",
    "cleaned_results.sort_values(by='Accuracy',ascending=False).drop(['Precision','Recall','F1-Score'],axis=1)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hidden_Layers</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>0.958824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>0.946479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>0.946176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>0.946023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.943182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Hidden_Layers  Precision\n",
       "33             35   0.958824\n",
       "30             32   0.946479\n",
       "12             14   0.946176\n",
       "16             18   0.946023\n",
       "5               7   0.943182"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Top 5 Models by Precision\n",
    "cleaned_results.sort_values(by='Precision',ascending=False).drop(['Accuracy','Recall','F1-Score'],axis=1)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hidden_Layers</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>0.869136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.864198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.859259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>0.859259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24</td>\n",
       "      <td>0.856790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Hidden_Layers    Recall\n",
       "29             31  0.869136\n",
       "7               9  0.864198\n",
       "2               4  0.859259\n",
       "9              11  0.859259\n",
       "22             24  0.856790"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Top 5 Models by Recall\n",
    "cleaned_results.sort_values(by='Recall',ascending=False).drop(['Accuracy','Precision','F1-Score'],axis=1)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hidden_Layers</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>0.890323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.887199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24</td>\n",
       "      <td>0.886335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.885496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>0.884422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Hidden_Layers  F1-Score\n",
       "36             38  0.890323\n",
       "7               9  0.887199\n",
       "22             24  0.886335\n",
       "2               4  0.885496\n",
       "29             31  0.884422"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Top 5 Models by F1-Score\n",
    "cleaned_results.sort_values(by='F1-Score',ascending=False).drop(['Accuracy','Precision','Recall'],axis=1)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing models for each category below (number of hidden layers in parentheses) were:\n",
    "* Accuracy = 0.981 from Model(38)\n",
    "* Precision = 0.959 from Model(35)\n",
    "* Recall = 0.869 from Model(31)\n",
    "* F1-Score = 0.890 from Model(38)\n",
    "\n",
    "The objective of this project is to find a model that can correctly identify pulsars, which account for only approximately 10% of the dataset. Therefore, the most important metric is precision for this exercise. The model with the highest precision had 35 hidden layers. Its evaluation metrics are saved below for future comparison with the other models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35.0,\n",
       " 0.9792178770949721,\n",
       " 0.9588235294117647,\n",
       " 0.8049382716049382,\n",
       " 0.8751677852348994]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation Metrics for Model with 35 Hidden Layers\n",
    "output_list = cleaned_results.loc[33].values.tolist()\n",
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"2020_1126_DNN_Results.csv\",\"w\") as file:\n",
    "    file.write('Model,Accuracy,Precision,Recall,F1-Score\\n')\n",
    "    file.write('DNN,0.9792178770949721,0.9588235294117647,0.8049382716049382,0.8751677852348994\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
